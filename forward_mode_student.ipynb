{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP-2 : Implémentation de l'Autodifférentiation en Mode Forward\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif de ce TP est d'implémenter l'autodifférentiation en mode forward en utilisant des nombres duals. Vous allez créer une classe Python qui représente un tenseur contenant une partie réelle et une partie duale, et implémenter les opérations arithmétiques de base (addition, soustraction, multiplication, division, puissance) ainsi que la différentiation automatique en mode forward.\n",
    "\n",
    "\n",
    "### Principe de la méthode d'autodifférentiation en mode forward\n",
    "\n",
    "L'autodifférentiation (ou différentiation automatique) est une technique utilisée pour calculer les dérivées de fonctions définies par des programmes informatiques. Contrairement aux méthodes de dérivation symbolique ou numérique, l'autodifférentiation est précise et efficace. Il existe deux modes principaux : le mode forward et le mode reverse.\n",
    "En mode forward, l'idée est de propager des paires de valeurs (la valeur de la fonction et la valeur de sa dérivée) à travers chaque opération élémentaire de la fonction. Voici comment cela fonctionne :\n",
    "\n",
    "1. **Initialisation** : On commence par associer à chaque variable d'entrée une paire de valeurs $(x_i, \\dot{x}_i)$, où $x_i$ est la valeur de la variable et $\\dot{x}_i$ est la dérivée de la variable par rapport à l'entrée d'intérêt (souvent initialisée à 1 pour la variable d'intérêt et à 0 pour les autres).\n",
    "\n",
    "2. **Propagation** : À chaque étape de la fonction, pour chaque opération élémentaire (comme l'addition, la multiplication, etc.), on calcule à la fois la nouvelle valeur de la fonction et la nouvelle valeur de la dérivée. Par exemple, pour une opération élémentaire $z = x + y$, on aurait :\n",
    "   - Valeur : $z = x + y$\n",
    "   - Dérivée : $\\dot{z} = \\dot{x} + \\dot{y}$\n",
    "   \n",
    "   Pour une multiplication $z = x \\cdot y$, on aurait :\n",
    "   - Valeur : $z = x \\cdot y$\n",
    "   - Dérivée : $\\dot{z} = \\dot{x} \\cdot y + x \\cdot \\dot{y}$\n",
    "\n",
    "3. **Résultat** : À la fin de la propagation, on obtient la valeur finale de la fonction ainsi que la valeur de sa dérivée par rapport à l'entrée d'intérêt.\n",
    "\n",
    "### Exemple simple\n",
    "\n",
    "Supposons que nous voulons dériver la fonction $f(x) = x^2 + 3x$ par rapport à $x$.\n",
    "\n",
    "1. **Initialisation** :\n",
    "   - $x = x$\n",
    "   - $\\dot{x} = 1$ (puisque nous dérivons par rapport à $x$)\n",
    "\n",
    "2. **Propagation** :\n",
    "   - Première opération : $u = x^2$\n",
    "     - $u = x \\cdot x$\n",
    "     - $\\dot{u} = \\dot{x} \\cdot x + x \\cdot \\dot{x} = 1 \\cdot x + x \\cdot 1 = 2x$\n",
    "   - Deuxième opération : $v = 3x$\n",
    "     - $v = 3 \\cdot x$\n",
    "     - $\\dot{v} = 3 \\cdot \\dot{x} = 3 \\cdot 1 = 3$\n",
    "   - Troisième opération : $f = u + v$\n",
    "     - $f = u + v$\n",
    "     - $\\dot{f} = \\dot{u} + \\dot{v} = 2x + 3$\n",
    "\n",
    "3. **Résultat** :\n",
    "   - La valeur de la fonction est $f(x) = x^2 + 3x$\n",
    "   - La dérivée de la fonction est $\\dot{f} = 2x + 3$\n",
    "\n",
    "Bon travail et bonne programmation !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préliminaire, surcharge des opérateurs et plot du graphe de calcul\n",
    "\n",
    "1. Observer et décrire la classe et le first_test\n",
    "2. Remarquer comment la surcharge d'opérateur permet de propager le graphe de calcul\n",
    "2. Adapter la classe pour rendre le second_test fonctionnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class TensorSimple:\n",
    "    graph = nx.DiGraph()  # Create a directed graph\n",
    "\n",
    "    def __init__(self, data, name=None):\n",
    "        self.data = np.array(data)\n",
    "        self.name = name\n",
    "        TensorSimple.graph.add_node(self)  # Add the current tensor as a node in the graph\n",
    "        self.operation = None\n",
    "        self.result = None\n",
    "\n",
    "    def __add__(self, other):\n",
    "        result = TensorSimple(self.data + other.data, name=f\"({self.name}+{other.name})\")\n",
    "        self._add_edge(other, result, '+')\n",
    "        return result\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        result = TensorSimple(self.data - other.data, name=f\"({self.name}-{other.name})\")\n",
    "        self._add_edge(other, result, '-')\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        result = TensorSimple(self.data * other.data, name=f\"({self.name}*{other.name})\")\n",
    "        self._add_edge(other, result, '*')\n",
    "        return result\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        result = TensorSimple(self.data / other.data, name=f\"({self.name}/{other.name})\")\n",
    "        self._add_edge(other, result, '/')\n",
    "        return result\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        result = TensorSimple(self.data ** other.data, name=f\"({self.name}^{other.name})\")\n",
    "        self._add_edge(other, result, '^')\n",
    "        return result\n",
    "\n",
    "    def _add_edge(self, other, result, operation):\n",
    "        TensorSimple.graph.add_edge(self, result, operation=operation)\n",
    "        TensorSimple.graph.add_edge(other, result, operation=operation)\n",
    "        result.operation = operation\n",
    "        result.result = result\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.data)\n",
    "\n",
    "\n",
    "def plot_graph(graph):\n",
    "    pos = nx.spring_layout(graph)\n",
    "    labels = {node: f\"{node.name}\\n{node.data}\" for node in graph.nodes}\n",
    "    edge_labels = {(u, v): f\"{d['operation']}\" for u, v, d in graph.edges(data=True)}\n",
    "    nx.draw(graph, pos, with_labels=True, labels=labels, node_size=2000, node_color='lightblue')\n",
    "    nx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def first_test():\n",
    "\n",
    "    TensorSimple.graph = nx.DiGraph()\n",
    "\n",
    "    # Create some tensors\n",
    "    a = TensorSimple(2, name='a')\n",
    "    b = TensorSimple(3, name='b')\n",
    "    c = TensorSimple(4, name='c')\n",
    "\n",
    "    # Perform some operations\n",
    "    d = a + b\n",
    "    e = d * c\n",
    "    f = e / a\n",
    "\n",
    "    # Draw the computational graph\n",
    "    plot_graph(TensorSimple.graph)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    first_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodifférentiation en mode forward\n",
    "\n",
    "Vous allez implémenter une classe `Tensor` qui représente un nombre dual. Chaque `Tensor` contient deux attributs :\n",
    "- `real` : la partie réelle du nombre.\n",
    "- `dual` : un dictionnaire représentant la partie duale, où les clés sont des indices nommés et les valeurs sont les valeurs duales correspondantes.\n",
    "\n",
    "### Méthodes à Implémenter\n",
    "\n",
    "1. **Constructeur (`__init__`)** :\n",
    "    - Initialise un `Tensor` avec une partie réelle et une partie duale.\n",
    "\n",
    "2. **Addition (`__add__` et `__radd__`)** :\n",
    "    - Permet l'addition de deux tenseurs ou l'addition d'un scalaire à un tenseur.\n",
    "\n",
    "3. **Soustraction (`__sub__` et `__rsub__`)** :\n",
    "    - Permet la soustraction de deux tenseurs ou la soustraction d'un scalaire d'un tenseur.\n",
    "\n",
    "4. **Multiplication (`__mul__` et `__rmul__`)** :\n",
    "    - Permet la multiplication de deux tenseurs ou la multiplication d'un scalaire par un tenseur.\n",
    "\n",
    "5. **Division (`__truediv__` et `__rtruediv__`)** :\n",
    "    - Permet la division de deux tenseurs ou la division d'un scalaire par un tenseur.\n",
    "\n",
    "6. **Puissance (`__pow__`)** :\n",
    "    - Permet d'élever un tenseur à une puissance donnée.\n",
    "\n",
    "7. **Négation (`__neg__`)** :\n",
    "    - Permet de négativer un tenseur.\n",
    "\n",
    "8. **Méthode auxiliaire `div_neg`** :\n",
    "    - Permet de négativer la partie duale d'un tenseur.\n",
    "\n",
    "9. **Représentation en chaîne (`__str__`)** :\n",
    "    - Retourne une représentation en chaîne de caractères du tenseur.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Implémentez la classe `Tensor` avec les méthodes décrites ci-dessus.\n",
    "2. Testez chaque méthode avec des exemples concrets pour vérifier leur bon fonctionnement.\n",
    "3. Documentez votre code et commentez chaque méthode pour expliquer son fonctionnement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "\n",
    "    def __init__(self, real, dual):\n",
    "        '''\n",
    "        real: real number\n",
    "        dual: dict (key=name_index and value=value)\n",
    "        '''\n",
    "        self.real = real  # The real part of the dual number\n",
    "        self.dual = dual  # The dual part of the dual number\n",
    "\n",
    "    def __add__(self, argument):\n",
    "\n",
    "\n",
    "        if isinstance(argument, Tensor):\n",
    "        \n",
    "            real = self.real + argument.real\n",
    "\n",
    "            dual = {}\n",
    "            for key in self.dual:\n",
    "                dual[key] = self.dual[key]\n",
    "            \n",
    "            for key in argument.dual:\n",
    "                if key in dual:\n",
    "                    dual[key] += argument.dual[key]\n",
    "                else:\n",
    "                    dual[key] = argument.dual[key]\n",
    "\n",
    "            return Tensor(real, dual)\n",
    "        \n",
    "\n",
    "        else:\n",
    "            return Tensor(self.real + argument, self.dual)\n",
    "        \n",
    "    __radd__ = __add__\n",
    "    \n",
    "    def __mul__(self, argument):\n",
    "\n",
    "        ## Tensor\n",
    "        if isinstance(argument, Tensor):\n",
    "            real = self.real * argument.real\n",
    "\n",
    "            dual = {}\n",
    "            for key in self.dual:\n",
    "                dual[key] = self.dual[key] * argument.real\n",
    "\n",
    "            for key in argument.dual:\n",
    "                if key in dual:\n",
    "                    dual[key] += argument.dual[key] * self.real\n",
    "                else:\n",
    "                    dual[key] = argument.dual[key] * self.real\n",
    "\n",
    "            return Tensor(real, dual)\n",
    "        \n",
    "        else: ## Constante\n",
    "            real = self.real * argument\n",
    "            dual = {}\n",
    "            for key in self.dual:\n",
    "                dual[key] = self.dual[key] * argument\n",
    "            return Tensor(real, dual)\n",
    "        \n",
    "    __rmul__ = __mul__\n",
    "    \n",
    "    def __sub__(self, argument):\n",
    "        \n",
    "        if (isinstance(argument, Tensor)):\n",
    "            real = self.real - argument.real\n",
    "\n",
    "            dual = {}\n",
    "            for key in self.dual:\n",
    "                dual[key] = self.dual[key] - argument.dual.get(key, 0)\n",
    "            \n",
    "            for key in argument.dual:\n",
    "                if key not in dual:\n",
    "                    dual[key] = -argument.dual[key]\n",
    "\n",
    "            return Tensor(real, dual)\n",
    "        else:\n",
    "            return Tensor(self.real - argument, self.dual)    \n",
    "        \n",
    "    def __rsub__(self, argument):\n",
    "        result = Tensor(argument, {}) - self\n",
    "        return result\n",
    "    \n",
    "    def __truediv__(self, argument):\n",
    "            ## Tensor\n",
    "        if isinstance(argument, Tensor):\n",
    "            # calcul de la partie réelle\n",
    "            real = self.real / argument.real\n",
    "\n",
    "            # calcul de la partie duale\n",
    "            dual = {}\n",
    "            for key in self.dual:\n",
    "                dual[key] = self.dual[key] / argument.real\n",
    "\n",
    "            for key in argument.dual:\n",
    "                if key in dual:\n",
    "                    dual[key] -= (self.real * argument.dual[key]) / (argument.real ** 2)\n",
    "                else:\n",
    "                    dual[key] = -(self.real * argument.dual[key]) / (argument.real ** 2)\n",
    "\n",
    "            return Tensor(real, dual)\n",
    "      \n",
    "    def __rtruediv__(self,argument):\n",
    "        result = Tensor(argument, {}) / self\n",
    "        return result\n",
    "        \n",
    "  \n",
    "    def __pow__(self, power):\n",
    "        if isinstance(power, (int, float)):\n",
    "            real = self.real ** power\n",
    "            dual = {}\n",
    "            for key in self.dual:\n",
    "                dual[key] = self.dual[key] * power * (self.real ** (power - 1))\n",
    "\n",
    "            return Tensor(real, dual)\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __neg__(self):\n",
    "        real = -self.real\n",
    "        dual = {}\n",
    "        for key in self.dual:\n",
    "            dual[key] = -self.dual[key]\n",
    "        return Tensor(real, dual)\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = 'f = ' + str(round(self.real,6)) + '\\n'\n",
    "        for key in self.dual:\n",
    "            s += 'f' + key + ' = ' + str(round(self.dual[key],6)) + '\\n'\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Premier Test ---\n",
      "Wolf Forward Mode:\n",
      "f = 4\n",
      "fW = 2\n",
      "fb = 1\n",
      "\n",
      "PyTorch:\n",
      "cost: 4.0\n",
      "grad W: 2.0\n",
      "grad b: 1.0\n",
      "\n",
      "--- Second Test ---\n",
      "Wolf Forward Mode:\n",
      "f = 72\n",
      "fW = 42\n",
      "fb = 9\n",
      "\n",
      "PyTorch:\n",
      "cost: 72.0\n",
      "grad W: 42.0\n",
      "grad b: 9.0\n"
     ]
    }
   ],
   "source": [
    "### Test d'autodifférentiation pour l'addition\n",
    "import torch\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    ## Test the class\n",
    "    # f(x) = aW + b\n",
    "    # c(x,y) = f(x) - y    \n",
    "    \n",
    "    ## Parameters\n",
    "    W = Tensor(3, {'W': 1})\n",
    "    b = Tensor(2, {'b': 1})\n",
    "    \n",
    "    ## Constantes\n",
    "    x = 2\n",
    "    y = 4\n",
    "\n",
    "    f = x*W + b\n",
    "    cost = f - y\n",
    "    print(\"--- Premier Test ---\")\n",
    "    print(\"Wolf Forward Mode:\")\n",
    "    print(cost)\n",
    "\n",
    "    # PyTorch comparison\n",
    "    W_torch = torch.tensor(3.0, requires_grad=True)\n",
    "    b_torch = torch.tensor(2.0, requires_grad=True)\n",
    "    x_torch = 2.0\n",
    "    y_torch = 4.0\n",
    "\n",
    "    f_torch = x_torch * W_torch + b_torch\n",
    "    cost_torch = f_torch - y_torch\n",
    "    cost_torch.backward()\n",
    "    \n",
    "    print(\"PyTorch:\")\n",
    "    print(f\"cost: {cost_torch.item()}\")\n",
    "    print(f\"grad W: {W_torch.grad.item()}\")\n",
    "    print(f\"grad b: {b_torch.grad.item()}\")\n",
    "    print()\n",
    "\n",
    "    ############################\n",
    "\n",
    "    ## Constantes\n",
    "    x_anchor = 2\n",
    "    x_pos = 4\n",
    "    x_neg = 1\n",
    "\n",
    "    f_anchor = x_anchor*W + b\n",
    "    f_pos = x_pos*W + b\n",
    "    f_neg = x_neg*W + b\n",
    "\n",
    "    cost =  (f_anchor * f_pos) - (f_anchor * f_neg)\n",
    "    print(\"--- Second Test ---\")\n",
    "    print(\"Wolf Forward Mode:\")\n",
    "    print(cost)\n",
    "\n",
    "    # PyTorch comparison\n",
    "    W_torch.grad.zero_()\n",
    "    b_torch.grad.zero_()\n",
    "\n",
    "    f_anchor_torch = x_anchor * W_torch + b_torch\n",
    "    f_pos_torch = x_pos * W_torch + b_torch\n",
    "    f_neg_torch = x_neg * W_torch + b_torch\n",
    "\n",
    "    cost_complex_torch = (f_anchor_torch * f_pos_torch) - (f_anchor_torch * f_neg_torch)\n",
    "    cost_complex_torch.backward()\n",
    "\n",
    "    print(\"PyTorch:\")\n",
    "    print(f\"cost: {cost_complex_torch.item()}\")\n",
    "    print(f\"grad W: {W_torch.grad.item()}\")\n",
    "    print(f\"grad b: {b_torch.grad.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions Mathématiques avec des Nombres Duals pour l'Autodifférentiation\n",
    "\n",
    "### Objectif\n",
    "\n",
    "L'objectif de cette partie du TP est d'implémenter des fonctions mathématiques couramment utilisées dans le contexte de l'autodifférentiation en mode forward en utilisant des nombres duals. Vous allez créer des fonctions qui calculent à la fois la valeur d'une fonction mathématique et sa dérivée, représentée par la partie duale.\n",
    "Vous allez implémenter plusieurs fonctions mathématiques en utilisant des nombres duals représentés par la classe `Tensor`. Chaque fonction doit calculer la valeur de la fonction et la dérivée, et renvoyer un objet `Tensor` contenant ces informations.\n",
    "\n",
    "### Fonctions à Implémenter\n",
    "\n",
    "1. **Logarithme naturel (`log_d`)** :\n",
    "    - Calcule la valeur du logarithme naturel et sa dérivée.\n",
    "\n",
    "2. **Exponentielle (`exp_d`)** :\n",
    "    - Calcule la valeur de l'exponentielle et sa dérivée.\n",
    "\n",
    "3. **Sinus (`sin_d`)** :\n",
    "    - Calcule la valeur du sinus et sa dérivée.\n",
    "\n",
    "4. **Cosinus (`cos_d`)** :\n",
    "    - Calcule la valeur du cosinus et sa dérivée.\n",
    "\n",
    "5. **Sigmoïde (`sigmoid_d`)** :\n",
    "    - Calcule la valeur de la fonction sigmoïde et sa dérivée.\n",
    "\n",
    "6. **Tangente hyperbolique (`tanh_d`)** :\n",
    "    - Calcule la valeur de la tangente hyperbolique et sa dérivée.\n",
    "\n",
    "7. **Tangente (`tan_d`)** :\n",
    "    - Calcule la valeur de la tangente et sa dérivée.\n",
    "\n",
    "8. **Racine carrée (`sqrt_d`)** :\n",
    "    - Calcule la valeur de la racine carrée et sa dérivée.\n",
    "\n",
    "9. **Puissance (`pow_d`)** :\n",
    "    - Calcule la valeur de la fonction puissance et sa dérivée.\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Implémentez les fonctions décrites ci-dessus.\n",
    "2. Testez chaque fonction avec des exemples concrets pour vérifier leur bon fonctionnement.\n",
    "3. Documentez votre code et commentez chaque fonction pour expliquer son fonctionnement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_d(dual_number):\n",
    "    dual = {}\n",
    "    a = dual_number.real\n",
    "    sa = np.sin(a)\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] * np.cos(a)\n",
    "    return Tensor(sa, dual)\n",
    "\n",
    "def cos_d(dual_number):\n",
    "    dual = {}\n",
    "    a = dual_number.real\n",
    "    sa = np.cos(a)\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] * (-np.sin(a))\n",
    "    return Tensor(sa, dual)\n",
    "\n",
    "def log_d(dual_number):    \n",
    "    dual = {}\n",
    "    a = dual_number.real\n",
    "    sa = np.log(a)\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] / a\n",
    "    return Tensor(sa, dual)\n",
    "\n",
    "def exp_d(dual_number):\n",
    "    dual = {}\n",
    "    a = dual_number.real\n",
    "    sa = np.exp(a)\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] * np.exp(a)\n",
    "    return Tensor(sa, dual)\n",
    "\n",
    "def sigmoid_d(dual_number):\n",
    "    dual = {}\n",
    "    a = dual_number.real\n",
    "    sa = 1 / (1 + np.exp(-a))\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] * sa * (1 - sa)\n",
    "    return Tensor(sa, dual)\n",
    "\n",
    "def tanh_d(dual_number):\n",
    "    dual = {}\n",
    "    a = dual_number.real\n",
    "    sa = np.tanh(a)\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] * (1 - np.tanh(a)**2)\n",
    "    return Tensor(sa, dual)\n",
    "\n",
    "def tan_d(dual_number):\n",
    "    dual = {}\n",
    "    a = dual_number.real\n",
    "    sa = np.tan(a)\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] * (1 / np.cos(a))**2\n",
    "    return Tensor(sa, dual)\n",
    "\n",
    "\n",
    "def sqrt_d(dual_number):\n",
    "    dual = {}\n",
    "    a = dual_number.real\n",
    "    sa = np.sqrt(a)\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] * (1 / (2 * np.sqrt(a)))\n",
    "    return Tensor(sa, dual)\n",
    "\n",
    "def pow_d(dual_number, power):\n",
    "    dual = {}\n",
    "    \n",
    "    a = dual_number.real\n",
    "    sa = np.power(a, power)\n",
    "\n",
    "    for key in dual_number.dual:\n",
    "        dual[key] = dual_number.dual[key] * power * np.power(a, power - 1)\n",
    "    return Tensor(sa, dual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests Principaux et Comparaison avec PyTorch\n",
    "\n",
    "Dans les cellules suivantes, nous allons définir une série de fonctions mathématiques de complexité variable :\n",
    "- Fonctions polynomiales\n",
    "- Fonctions trigonométriques (sin, cos, tan)\n",
    "- Fonctions exponentielles et logarithmiques\n",
    "- Fonctions composées\n",
    "\n",
    "L'objectif est de tester la robustesse de votre implémentation. Pour chaque fonction, nous calculerons le gradient en utilisant votre classe `Tensor` (Mode Forward) et nous comparerons le résultat avec le gradient calculé par la librairie **PyTorch**, qui servira de référence (ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define the function for which we want to compute the gradient\n",
    "def f(x):\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "# Cubic function\n",
    "def f7(x):\n",
    "    return x**3 + 3*x**2 - 2*x + 5\n",
    "\n",
    "# Quartic function\n",
    "def f8(x):\n",
    "    return x**4 + 4*x**3 - 3*x**2 + 6*x + 7\n",
    "\n",
    "# Quintic function\n",
    "def f9(x):\n",
    "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + 10\n",
    "\n",
    "# Exponential function\n",
    "def f10(x):\n",
    "    return exp_d(x)\n",
    "\n",
    "def f10_torch(x):\n",
    "    return torch.exp(x)\n",
    "\n",
    "# Logarithmic function\n",
    "def f11(x):\n",
    "    return log_d(x)\n",
    "\n",
    "def f11_torch(x):\n",
    "    return torch.log(x)\n",
    "\n",
    "# Sinusoidal function\n",
    "def f12(x):\n",
    "    return sin_d(x)\n",
    "\n",
    "def f12_torch(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "# Cosinusoidal function\n",
    "def f13(x):\n",
    "    return cos_d(x)\n",
    "\n",
    "def f13_torch(x):\n",
    "    return torch.cos(x)\n",
    "\n",
    "# Tangent function\n",
    "def f14(x):\n",
    "    return tan_d(x)\n",
    "\n",
    "def f14_torch(x):\n",
    "    return torch.tan(x)\n",
    "\n",
    "# Hyperbolic tangent function\n",
    "def f17(x):\n",
    "    return tanh_d(x)\n",
    "\n",
    "def f17_torch(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "# Inverse function\n",
    "def f18(x):\n",
    "    return 1/x\n",
    "\n",
    "# Square root function\n",
    "def f19(x):\n",
    "    return sqrt_d(x)\n",
    "\n",
    "def f19_torch(x):\n",
    "    return torch.sqrt(x)\n",
    "\n",
    "# Natural logarithm function\n",
    "def f22(x):\n",
    "    return log_d(x)\n",
    "\n",
    "def f22_torch(x):\n",
    "    return torch.log(x)\n",
    "\n",
    "# Define the function for which we want to compute the gradient\n",
    "def f(x):\n",
    "    return x**2 + 2*x + 1\n",
    "\n",
    "# Cubic function with sin\n",
    "def f27(x):\n",
    "    return x**3 + 3*x**2 - 2*x + sin_d(x) + 5\n",
    "\n",
    "def f27_torch(x):\n",
    "    return x**3 + 3*x**2 - 2*x + torch.sin(x) + 5\n",
    "\n",
    "# Quartic function with cos\n",
    "def f28(x):\n",
    "    return x**4 + 4*x**3 - 3*x**2 + 6*x + cos_d(x) + 7\n",
    "\n",
    "def f28_torch(x):\n",
    "    return x**4 + 4*x**3 - 3*x**2 + 6*x + torch.cos(x) + 7\n",
    "\n",
    "# Quintic function with sin and cos\n",
    "def f29(x):\n",
    "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + sin_d(x) + cos_d(x) + 10\n",
    "\n",
    "def f29_torch(x):\n",
    "    return x**5 + 5*x**4 - 4*x**3 + 8*x**2 + 9*x + torch.sin(x) + torch.cos(x) + 10\n",
    "\n",
    "# Function with sin^2 and cos^2\n",
    "def f30(x):\n",
    "    return sin_d(x)**2 + cos_d(x)**2 + x**2\n",
    "\n",
    "def f30_torch(x):\n",
    "    return torch.sin(x)**2 + torch.cos(x)**2 + x**2\n",
    "\n",
    "# Function with sin^3 and cos^3\n",
    "def f31(x):\n",
    "    return sin_d(x)**3 + cos_d(x)**3 + x**3\n",
    "\n",
    "def f31_torch(x):\n",
    "    return torch.sin(x)**3 + torch.cos(x)**3 + x**3\n",
    "\n",
    "### define a list of functions\n",
    "functions = [f, f7, f8, f9, f10, f11, f12, f13, f14, f17, f18, f19, f22, f27, f28, f29, f30, f31]\n",
    "functions_torch = [f, f7, f8, f9, f10_torch, f11_torch, f12_torch, f13_torch, f14_torch, f17_torch, f18, f19_torch, f22_torch, f27_torch, f28_torch, f29_torch, f30_torch, f31_torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient computed by Forward Mode: {'x': 2.2}\n",
      "Gradient computed by PyTorch: 2.200000047683716\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': -1.3699999999999999}\n",
      "Gradient computed by PyTorch: -1.3700000047683716\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': 5.524}\n",
      "Gradient computed by PyTorch: 5.52400016784668\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': 10.5005}\n",
      "Gradient computed by PyTorch: 10.500500679016113\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(1.1051709180756477)}\n",
      "Gradient computed by PyTorch: 1.1051709651947021\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': 10.0}\n",
      "Gradient computed by PyTorch: 10.0\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(0.9950041652780258)}\n",
      "Gradient computed by PyTorch: 0.9950041770935059\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(-0.09983341664682815)}\n",
      "Gradient computed by PyTorch: -0.0998334214091301\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(1.0100670464224946)}\n",
      "Gradient computed by PyTorch: 1.010067105293274\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(0.9900662908474398)}\n",
      "Gradient computed by PyTorch: 0.9900662899017334\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': -99.99999999999999}\n",
      "Gradient computed by PyTorch: -100.0\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(1.5811388300841895)}\n",
      "Gradient computed by PyTorch: 1.5811388492584229\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': 10.0}\n",
      "Gradient computed by PyTorch: 10.0\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(-0.37499583472197406)}\n",
      "Gradient computed by PyTorch: -0.37499579787254333\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(5.424166583353172)}\n",
      "Gradient computed by PyTorch: 5.424166679382324\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(11.395670748631199)}\n",
      "Gradient computed by PyTorch: 11.395671844482422\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(0.2)}\n",
      "Gradient computed by PyTorch: 0.20000000298023224\n",
      "\n",
      "Gradient computed by Forward Mode: {'x': np.float64(-0.23676446036681106)}\n",
      "Gradient computed by PyTorch: -0.23676446080207825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a tensor with a single value\n",
    "    x = Tensor(0.1, {'x': 1})\n",
    "\n",
    "    for f, f_torch in zip(functions, functions_torch):\n",
    "        # Compute the gradient using Wolf Forward Mode\n",
    "        x_prime = f(x)\n",
    "        print(\"Gradient computed by Forward Mode:\", x_prime.dual)\n",
    "\n",
    "        # Convert the tensor to a PyTorch tensor\n",
    "        x_torch = torch.tensor(x.real)\n",
    "        x_torch.requires_grad_(True)\n",
    "        y_torch = f_torch(x_torch)\n",
    "        y_torch.backward()\n",
    "        print(\"Gradient computed by PyTorch:\", x_torch.grad.item())\n",
    "        print()\n",
    "        \n",
    "        #assert np.isclose(x_prime.dual['x'], x_torch.grad.item(), atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test d'autodifférentiation avec deux variables\n",
    "\n",
    "Dans cette section, nous étendons les tests à des fonctions dépendant de deux variables (x et y). L'objectif est de vérifier que votre implémentation gère correctement les dérivées partielles.\n",
    "\n",
    "Nous allons définir des fonctions de deux variables de complexité croissante :\n",
    "- Fonctions polynomiales simples\n",
    "- Fonctions combinant polynômes et fonctions trigonométriques\n",
    "- Fonctions composées utilisant sin et cos\n",
    "\n",
    "Pour chaque fonction, nous calculerons les gradients partiels par rapport à x et y en utilisant votre classe `Tensor` (Mode Forward), puis nous comparerons avec les gradients calculés par **PyTorch** pour valider l'exactitude des résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Quartic function with cos\n",
    "def f1(x, y):\n",
    "    return x**4 + 6*y \n",
    "\n",
    "def f1_torch(x, y):\n",
    "    return x**4 + 6*y\n",
    "\n",
    "# Quintic function with sin and cos\n",
    "def f2(x, y):\n",
    "    return x**5 + 5*y**4 - 4*x**3 + 8*x**2 + 9*x + sin_d(y) + cos_d(x) + 10\n",
    "\n",
    "def f2_torch(x, y):\n",
    "    return x**5 + 5*y**4 - 4*x**3 + 8*x**2 + 9*x + torch.sin(y) + torch.cos(x) + 10\n",
    "\n",
    "# Function with sin^2 and cos^2\n",
    "def f3(x, y):\n",
    "    return sin_d(y)**2 + cos_d(x)**2 + y**3\n",
    "\n",
    "# Function with sin^2 and cos^2\n",
    "def f3_torch(x, y):\n",
    "    return torch.sin(y)**2 + torch.cos(x)**2 + y**3\n",
    "\n",
    "functions = [f1, f2, f3]\n",
    "functions_torch = [f1_torch, f2_torch, f3_torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient computed by Wolf Forward Mode: {'x': 0.004000000000000001, 'y': 6.0}\n",
      "Gradient computed by PyTorch - x.grad: 0.004000000189989805 y.grad: 6.0\n",
      "\n",
      "Gradient computed by Wolf Forward Mode: {'x': np.float64(10.380666583353172), 'y': np.float64(1.1400665778412415)}\n",
      "Gradient computed by PyTorch - x.grad: 10.380666732788086 y.grad: 1.1400666236877441\n",
      "\n",
      "Gradient computed by Wolf Forward Mode: {'y': np.float64(0.5094183423086505), 'x': np.float64(-0.19866933079506124)}\n",
      "Gradient computed by PyTorch - x.grad: -0.19866934418678284 y.grad: 0.5094183683395386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a tensor with a single value\n",
    "\n",
    "    for f, f_torch in zip(functions, functions_torch):\n",
    "\n",
    "        x = Tensor(0.1, {'x': 1.0})\n",
    "        y = Tensor(0.2, {'y': 1.0})\n",
    "\n",
    "        # Compute the gradient using Wolf Forward Mode\n",
    "        z = f(x, y)\n",
    "        print(\"Gradient computed by Wolf Forward Mode:\", z.dual)\n",
    "\n",
    "        # Convert the tensor to a PyTorch tensor\n",
    "        x_torch = torch.tensor(x.real)\n",
    "        x_torch.requires_grad_(True)\n",
    "\n",
    "        y_torch = torch.tensor(y.real)\n",
    "        y_torch.requires_grad_(True)\n",
    "\n",
    "        z_torch = f_torch(x_torch, y_torch)\n",
    "        z_torch.backward()\n",
    "        print(f\"Gradient computed by PyTorch - x.grad: {x_torch.grad.item()} y.grad: {y_torch.grad.item()}\")\n",
    "        print()\n",
    "        \n",
    "        #assert np.isclose(x_prime.dual['x'], x_torch.grad.item(), atol=1e-6)\n",
    "        #assert np.isclose(x_prime.dual['y'], y_torch.grad.item(), atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réseau de neurones multi-couche simple\n",
    "\n",
    "Dans cette section, nous appliquons l'autodifférentiation à un réseau de neurones simple : un perceptron multi-couches (MLP) avec une seule couche cachée.\n",
    "\n",
    "L'objectif est de :\n",
    "- Implémenter le passage avant (forward pass) du réseau en utilisant votre classe `Tensor`\n",
    "- Calculer les gradients des paramètres (poids et biais) par rapport à la sortie\n",
    "- Comparer les gradients obtenus avec ceux de **PyTorch** pour valider l'implémentation\n",
    "\n",
    "Le réseau aura une architecture simple : entrée → couche cachée (avec activation ReLU) → sortie. Nous testerons avec une entrée scalaire et observerons les gradients des paramètres W1, b1, W2, b2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient computed by Wolf Forward Mode: {'x': tensor(0.0300, grad_fn=<MulBackward0>)}\n",
      "Gradient computed by PyTorch: 0.030000001192092896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def func_nn_torch(x):\n",
    "    W1 = torch.tensor(0.1, requires_grad=True)\n",
    "    b1 = torch.tensor(0.2, requires_grad=True)\n",
    "    W2 = torch.tensor(0.3, requires_grad=True)\n",
    "    b2 = torch.tensor(0.4, requires_grad=True)\n",
    "\n",
    "    y = W2 * (W1 * x + b1) + b2\n",
    "    return y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a tensor with a single value\n",
    "    x = Tensor(0.1, {'x': 1})\n",
    "\n",
    "    # Compute the gradient using Wolf Forward Mode\n",
    "    y = func_nn_torch(x)\n",
    "    print(\"Gradient computed by Wolf Forward Mode:\", y.dual)\n",
    "\n",
    "    # Convert the tensor to a PyTorch tensor\n",
    "    x_torch = torch.tensor(x.real)\n",
    "    x_torch.requires_grad_(True)\n",
    "    y_torch = func_nn_torch(x_torch)\n",
    "    y_torch.backward()\n",
    "    print(\"Gradient computed by PyTorch:\", x_torch.grad.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descente de gradient stochastique sur un réseau de neurones multi-couches\n",
    "\n",
    "Dans cette section finale, nous mettons en œuvre l'optimisation complète d'un réseau de neurones en utilisant la descente de gradient stochastique (SGD).\n",
    "\n",
    "L'objectif est de :\n",
    "- Générer des données synthétiques pour un problème de régression linéaire\n",
    "- Implémenter la fonction de perte (MSE - Mean Squared Error)\n",
    "- Entraîner le réseau en utilisant les gradients calculés par autodifférentiation\n",
    "- Observer la convergence de la perte au fil des époques\n",
    "\n",
    "Nous utiliserons un taux d'apprentissage initial qui décroît exponentiellement pour améliorer la convergence. Cette section démontre l'application pratique de votre implémentation d'autodifférentiation dans un contexte d'apprentissage automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4551767295627172\n",
      "0.522102306175796\n",
      "0.24889589055406147\n",
      "0.23334047666132707\n",
      "0.23185877185732992\n",
      "0.23171131083648533\n",
      "0.23169657180109005\n",
      "0.2316950979681892\n",
      "0.23169495058560585\n",
      "0.23169493584735446\n",
      "0.23169493437352973\n",
      "0.2316949342261481\n",
      "0.23169493421140902\n",
      "0.23169493420993414\n",
      "0.23169493420978757\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n",
      "0.23169493420977244\n"
     ]
    }
   ],
   "source": [
    "def func_nn(x, W1, b1, W2, b2):\n",
    "    # FIXME: Implement the forward pass of the neural network\n",
    "    y = W2 * (W1 * x + b1) + b2\n",
    "    return y\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    loss = (y - y_hat)**2.0\n",
    "    return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ## generate data\n",
    "    np.random.seed(0)\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = 2*x + 1 + np.random.randn(100)*0.1\n",
    "\n",
    "    ## Parameters\n",
    "    W1 = Tensor(0.1, {'W1': 1})\n",
    "    b1 = Tensor(0.2, {'b1': 1})\n",
    "    W2 = Tensor(0.3, {'W2': 1})\n",
    "    b2 = Tensor(0.4, {'b2': 1})\n",
    "    \n",
    "    lr = 0.01\n",
    "    nb_epoch = 100\n",
    "    for _ in range(nb_epoch):\n",
    "        \n",
    "        lst_loss = []\n",
    "        for i in range(len(x)):\n",
    "            x_i = Tensor(x[i], {'x': 1})\n",
    "            y_i = Tensor(y[i], {'y': 1})\n",
    "\n",
    "            y_hat = func_nn(x_i, W1, b1, W2, b2)\n",
    "            loss = mse(y_i, y_hat)\n",
    "            lst_loss.append(loss.real)\n",
    "\n",
    "            # Compute the gradient\n",
    "            W1 = W1 - lr*loss.dual['W1']\n",
    "            b1 = b1 - lr*loss.dual['b1']\n",
    "            W2 = W2 - lr*loss.dual['W2']\n",
    "            b2 = b2 - lr*loss.dual['b2']\n",
    "\n",
    "        print(np.mean(lst_loss))\n",
    "\n",
    "        # learning rate decay\n",
    "        lr *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prog-diff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
